##### Ch2 多臂老虎机

1. **行动价值**：选择行动$a$后的期望激励$q_*(a)$：
   $$
   q_*(a) \dot= \mathbb E[R_t\mid A_t=a]
   $$

2. **行动-价值方法**：记行动$a$在时间$t$的估计价值为$Q_t(a)$，则
   $$
   Q_t(a) \dot= \frac{\text{迄时间t采取a获得激励的总和}}{\text{迄时间t采取a的次数}} = \frac{\sum_{i=1}^{t-1} R_i \bullet\mathbf 1_{A_i=a}}{\sum_{i=1}^{t=1} \mathbf 1_{A_i=a}}
   $$
   $\mathbf 1_{predicate}$为预测为真时1否则0的随机变量；若分母为0，则设$Q_t(a)$为一默认值，如$Q_1(a) = 0$。

3. **10臂试验台**：行动价值$q_*(a), a=1,\dots,10$由均值为0方差为1的正态分布产生；在时间$t$选择行动$A_t$后，激励值$R_t$由均值为$q_*(A_t)$方差为1的正态分布产生。

4. **贪婪与$\varepsilon$贪婪**：若某个行动激励的方差变大，适于$\varepsilon$-贪婪；若方差为0，则贪婪表现最好，但若任务非平稳，也需要探索。

5. **增量实现**：令$R_i$表示所评估的行动在第$i$次选中后收到的激励，$Q_n$表示被选中$n-1$次后价值的估计，即$Q_n \dot= \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}$，可以改为增量形式来提高效率：
   $$
   Q_{n+1} = \frac{1}{n} \sum_{i=1}^n R_i  = Q_n + \frac{1}{n} \left[ R_n - Q_n \right]
   $$

6. 使用增量计算样本均值和$\varepsilon$贪婪行为选择的完整老虎机算法：
   $$
   \bbox[25px,border:2px solid]
   {\begin{aligned}
   &\underline{\mathbf{A\space simple\space bandit\space algorithm}}\\
   \\
   &\text{Initialize, for }a=1\text{ to }k\text{:}\\
   &\qquad Q(a) \leftarrow 0\\
   &\qquad N(a) \leftarrow 0\\
   \\
   &\text{Repeat forever:}\\
   &\qquad A \leftarrow 
   \begin{cases}
   \arg\max_a Q(a)&\qquad\text{with probability }1-\varepsilon\\
   \text{a random action}&\qquad\text{with probability }\varepsilon
   \end{cases}\\
   &\qquad R \leftarrow bandit(A)\\
   &\qquad N(A) \leftarrow N(A) + 1\\
   &\qquad Q(A) \leftarrow Q(A) + \frac{1}{N(A)}[R-Q(A)]
   \end{aligned}}
   $$

7. 用**常量步长**处理**非平稳**问题：非平稳环境中行动的真值随时间变化，可以给近期的激励赋予更多权重，即加权平均，最普遍的一种做法是使用固定步长参数，也称指数新近加权均值：
   $$
   Q_{n+1} \dot= Q_n + \alpha \left[ R_n - Q_n \right]=(1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
   $$
   步长参数$\alpha \in (0,1]^1$为常量，权重之和$(1-\alpha)^n + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}=1$。

8. **步长参数随步数改变**：令$\alpha_n(a)$表示$n$次选择行动$a$后用于计算激励的步长参数，在随机逼近理论中，保证以概率1收敛到真实行动价值的条件是：
   $$
   \sum_{n=1}^\infty \alpha_n(a) = \infty\qquad\text{and}\qquad\sum_{n=1}^\infty \alpha_n^2(a) < \infty
   $$
   第一个条件保证了步长足够大到能逐渐克服任何初始条件或随机波动，这二个条件保证了步长逐渐变得足够小到能收敛。采样-平均的步长参数$\alpha_n(a) = \frac{1}{n}$两个收敛条件都满足；常量步长$\alpha_n(a)=\alpha$不满足第二个条件。





##### Ch3 有限马尔科夫过程

1. **$t$时刻的回报的定义**：
   $$
   \begin{eqnarray*}
   G_t
   &=& \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} &\quad T=\infty\ \text{and}\ \gamma=1\ \text{not simultaneously}\\
   &=& R_{t+1} + \gamma G_{t+1} &\quad \text{Bellman equation}
   \end{eqnarray*}
   $$

2. **马尔科夫特性**：若信号具有马尔科夫特性，则环境在$t+1$时刻的反应仅依赖于在时间$t$状态和行为的表达：
   $$
   P(S_{t+1},R_{t+1}|S_0,A_0,R_1,S_1,A_1,\dots,R_t,S_t,A_t) = P(S_{t+1}, R_{t+1} \mid S_n, A_n)
   $$

3. **马尔科夫决策过程**：

   - 满足马尔科夫特性的强化学习任务；若状态和空间都有限，为有限MDP；
   - $p(s',r\mid s,a)$可完全指定一个有限MDP动态，计算有关环境的任何内容；

4. **策略$\pi$的状态-价值函数**，用于评估状态的好坏：
   $$
   v_{\pi}(s) \dot=\mathbb E_{\pi}[G_t | S_t=s] = \mathbb E_\pi \left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \middle| S_t=s \right]
   $$

5. **策略$\pi$的行为-价值函数**，用于评估某状态下某行为的好坏：
   $$
   q_{\pi}(s,a) \dot= \mathbb E_{\pi}[G_t \mid  S_t=t, A_t=a] = \mathbb E_{\pi}\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \middle | S_t=s, A_t=a \right]
   $$

6. **$v_\pi$的贝尔曼方程**：表述状态的价值与其后继价值的关系：
   $$
   v_\pi(s) = \sum_a \pi(a\mid s) \sum_{s',r} p(s',r\mid s,a)\left[ r+\gamma v_\pi(s') \right]\quad\forall s \in \mathcal S
   $$

7. **$q_\pi$的贝尔曼方程**：
   $$
   q_\pi(s,a) = \sum_{s',r}p(s',r\mid s,a) \left[ r + \gamma\sum_{a'}\pi(a'\mid s')q_\pi(s',a') \right]\quad \forall s\in\mathcal S, \forall a\in\mathcal A(s)
   $$

8. **$v_\pi$与$q_\pi$的关系**：
   $$
   \begin{eqnarray*}
   v_\pi(s) &=& \sum_a \pi(a\mid s) q_\pi(s,a)\\
   q_\pi(s,a) &=& \sum_{s',r}p(s',r\mid s,a)\left(r+\gamma v_\pi(s')\right)
   \end{eqnarray*}
   $$

9. **最优状态价值函数$v_*$**：
   $$
   v_*(s) \dot= \max_\pi v_\pi(s),\quad \forall s \in \mathcal S
   $$

10. **最优行为价值函数$q_*$**：
  $$
  q_*(s,a) \dot= \max_{\pi} q_\pi(s,a)\quad \forall s \in \mathcal S,\forall a\in \mathcal A(s)
  $$

11. **$v_*$最优贝尔曼性方程**：
    $$
    v_*(s) = \max_{a\in\mathcal A(s)} \sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma v_*(s') \right]
    $$

12. **$q_*$最优贝尔曼性方程**：
    $$
    q_*(s,a) = \sum_{s',r} p(s',r\mid s,a) \left[ r+\gamma\max_{a'}q_*(s',a') \right]
    $$

13. **$v_*$与$q_*$的关系**：
    $$
    \begin{eqnarray*}
    v_*(s) &=& \max_a q_*(s,a)\\
    q_*(s,a)&=& \max_{s'}\sum_r p(s',r\mid s,a)v_*(s')
    \end{eqnarray*}
    $$





##### Ch4 动态规划

1. **迭代策略评估**：若环境动态$p(s',r\mid s,a)$已知，则$v_\pi$的贝尔曼方程就是$\left\vert\mathcal S\right\vert$个$\left\vert\mathcal S\right\vert$元线性方程组，用迭代方法计算，$v_0, v_1, v_2,\cdots$为近似价值函数序列初始估计$v_0$可任意选择（除终止状态必须为0），对任意$s \in \mathcal S$:
   $$
   \begin{eqnarray*}
   v_{k+1}(s)
   &=& \mathbb E_\pi[R_{t+1}+\gamma v_k(S_{t+1}) \mid S_t=s] \\
   &=& \sum_a \pi(a\mid s)\sum_{s',r} p(s',r\mid s,a) \left[ r+\gamma v_k(s') \right]
   \end{eqnarray*}
   $$

2. 带**停机标准**的完整策略评估迭代算法为：
   $$
   \bbox[25px,border:2px solid]
   {
   \begin{aligned}
   &\underline{\mathbf{Iterative\space policy\space evaluation}}\\
   \\
   &\text{Input }\pi,\text{ the policy to be evaluated}\\
   &\text{Initialize an array }V(s)=0,\text{ for all }s\in\mathcal S^+\\
   &\text{Repeat}\\
   &\qquad\Delta \leftarrow 0\\
   &\qquad\text{For each }s\in\mathcal S:\\
   &\qquad\qquad v \leftarrow V(s)\\
   &\qquad\qquad V(s)\leftarrow\sum_a\pi(a\mid s)\sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma V(s') \right]\\
   &\qquad\qquad\Delta\leftarrow\max(\Delta,\left\vert v-V(s) \right\vert)\\
   &\text{until }\Delta < \theta\text{ (a small positive number) }\\
   &\text{Output }V\approx v_\pi
   \end{aligned}
   }
   $$

3. **策略改善定理**：若$\pi$和$\pi'$为任意策略对，若$\forall s \in \mathcal S$有
   $$
   q_\pi(s,\pi'(s)) \ge v_\pi(s)
   $$
   则策略$\pi'$必然与策略$\pi$等优或更优，即$\pi$必然从$\forall s \in \mathcal S$获得更多或相等的期望回报,
   $$
   v_\pi'(s)\ge v_\pi(s)
   $$
   若对任意状态前一个方程都是严格不等式，则后一个方程中至少有一个状态是严格的。要证明策略改善定理，从前一个不等式开始持续展开$q_\pi$侧并反复应用它直到获得$v_{\pi'}(s)$。

4. **策略改善**：已知策略及其价值函数，由策略改善定理，在每个状态使用贪婪策略——即选择每个状态$q_\pi(s,a)$最好的行为——来改善策略：
   $$
   \begin{eqnarray*}
   \pi'(s)
   &\dot=&  \arg\max_a q_\pi(s,a)\\
   &=& \arg\max_a \sum_{s',r} p(s',r\mid s,a)\left[ r+\gamma v_\pi(s') \right]
   \end{eqnarray*}
   $$
   改进后的策略$\pi'$，满足$\forall s \in \mathcal S$，
   $$
   \begin{eqnarray*}
   v_{\pi'}(s)
   &=& \max_a \mathbb E\left[ R_{t+1} + \gamma v_{\pi'}(S_{t+1}) \middle | S_t=s, A_t=a \right]\\
   &=& \max_a \sum_{s',r} p(s',r\mid s,a) \left[ r+\gamma v_{\pi'}(s') \right]
   \end{eqnarray*}
   $$
   满足贝尔曼最优性方程，因此$\pi'$必然是$\pi^*$。策略改善必然会给出严格更优的策略，除非原策略已是最优。

5. **策略迭代**：通过计算价值函数$v_\pi$改善策略$\pi$得到策略$\pi'$，又能计算$v_{\pi'}$产生更好的$\pi''$，获得一个单调递增的策略和价值函数序列：
   $$
   \begin{CD}
   \pi_0 @>E>> v_{\pi_0} @>I>> \pi_1 @>E>> v_{\pi_1} @>I>> \pi_2 @>E>> \cdots @>I>> \pi_* @>E>> v_*
   \end{CD}
   $$
   每个策略都保证为前一个的严格改善（除非已是最优），因有限MDP仅策略有限，它必然会在有限次迭代后收敛到最优策略和最优价值函数。完整算法如下：
   $$
   \bbox[25px,border:2px solid]
   {
   \begin{aligned}
   &\underline{\mathbf {Policy\space iteration}}\\
   \\
   1.&\text{Initialization}\\
   &V(s) \in \mathbb R\text{ and }\pi(s) \in \mathcal A(s)\text{ arbitrary for all }s\in \mathcal S\\
   \\
   2.&\text{Policy Evaluation}\\
   &\text{Repeat}\\
   &\qquad\Delta \leftarrow 0\\
   &\qquad\text{For each }s \in \mathcal S:\\
   &\qquad\qquad v\leftarrow V(s)\\
   &\qquad\qquad V(s) \leftarrow \sum_{s',r}p(s',r\mid s,\pi(s))\left[ r+\gamma V(s') \right]\\
   &\qquad\qquad \Delta\leftarrow \max(\Delta,\left\vert v-V(s) \right\vert)\\
   &\text{until }\Delta<\theta\text{ (a small positive number) }\\
   \\
   3.&\text{Policy Improvement}\\
   &policy\text-stable \leftarrow true\\
   &\text{For each }s\in\mathcal S:\\
   &\qquad old\text-action \leftarrow \pi(s)\\
   &\qquad \pi(s) \leftarrow \arg\max_{a} \sum_{s',r} p(s',r\mid s,a)\left[ r+\gamma V(s') \right]\\
   &\qquad \text{If } old\text-action\neq\pi(s),\text{ then }policy\text-stable \leftarrow false\\
   &\text{If }policy\text-stable,\text{ then stop and return }V\approx v_*\text{ and }\pi\approx\pi_*;\text{else go to 2}
   \end{aligned}
   }
   $$

6. **价值迭代**：在一次备份后就停止评估可以在不失去收敛保证下缩短策略迭代的策略评估，能够写为一个特别简单的结合策略改善和缩短策略评估的备份操作：
   $$
   \begin{eqnarray*}
   v_{k+1}(s) 
   &=& \max_a \mathbb E\left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t=s, A_t=a \right]\\
   &=& \max_a \sum_{s', r} p(s',r\mid s,a)\left[ r+\gamma v_k(s') \right]
   \end{eqnarray*}
   $$
   完整算法为：
   $$
   \bbox[25px,border:2px solid]
   {\begin{aligned}
   &\underline{\mathbf{Value\space iteration}}\\
   \\
   &\text{Initialize array }V\text{ arbitrarily}\\
   \\
   &\text{Repeat}\\
   &\qquad \Delta \leftarrow 0\\
   &\qquad\text{For each }s \in \mathcal S:\\
   &\qquad\qquad v \leftarrow V(s)\\
   &\qquad\qquad V(s) \leftarrow \max_a\sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma V(s') \right]\\
   &\qquad\qquad \Delta \leftarrow \max(\Delta, \left\vert v-V(s) \right\vert)\\
   &\text{until }\Delta < \theta\text{ (a small positive number)}\\
   &\\
   &\text{Output a deterministic policy, }\pi \approx\pi_*,\text{ such that}\\
   &\qquad\pi(s) = \arg\max_a \sum_{s',r} p(s',r\mid s,a)\left[ r + \gamma V(s') \right]
   \end{aligned}}
   $$

7. **异步动态规划**：

8. **广义**




##### Ch5 蒙特卡洛方法

1. **蒙特卡洛方法**：

   - 基于平均样本回报来解决强化学习问题的方法；
   - 仅为分节任务定义MC方法，因此经验被分为小节，而无论选择什么行为小节最终都会终止；
   - 仅当一个小节结束时，价值评估和策略才会改变，因此MC方法是**按节**递增；
   - MC方法对每个状态-行为对采样并求平均**回报**；
   - 因所有的行为选择都在学习中，从更早状态的观点来看这个问题就变得非平稳。

2. **蒙特卡洛预测**：

   - **每次访问MC方法**则将所有访问$s$后的回报求均值；

   - **首次访问MC方法**将$\pi_s$估计为首次访问$s$之后所有回报的均值，具体算法为：
     $$
     \bbox[5px,border:2px solid]
     {\begin{aligned}
       &\underline{\mathbf{First\text{-}visit\space MC\space prediction,\space for\space estimating\space }V\approx v_\pi}\\
       \\
       &\text{Initialize:}\\
       &\qquad \pi \leftarrow \text{policy to be evaluated}\\
       &\qquad V \leftarrow \text{an arbitrary state-value function}\\
       &\qquad Returns(s) \leftarrow \text{an empty list, for all }s\in \mathcal S\\
       \\
       &\text{Repeat forever:}\\
       &\qquad\text{Generate an episode using }\pi\\
       &\qquad\text{For each state }s\text{ appearing in the episode:}\\
       &\qquad\qquad G\leftarrow \text{return following the first occurrence of }s\\
       &\qquad\qquad\text{Append G to }Returns(s)\\
       &\qquad\qquad V(s) \leftarrow \text{average}(Returns(s))
       \end{aligned}}
     $$

   - 每个回报都是$v_\pi(s)$方差有界的独立同分布估计，因此随着首访次数趋于无穷，由大数定律这些评估的均值系列收敛到期望值，故MC收敛于$\pi(s)$。

   - 每个均值本身是一个无偏估计，其误差的标准差作$1/\sqrt{n}$下降，其中$n$是求均值的回报数（也就是说估计平方收敛）

3. **行为价值的MC估计**：

   - 若模型不可得，单状态价值函数就不是充分的，需要估计行为价值；
   - 行为价值$q_\pi(s,a)$评估的MC方法本质上与价值函数类似；
   - 也有每访MC和首访MC，随着访问状态-行为对次数趋于无穷，也二次地收敛；
   - **探索启动**假设：**将节指定行为-状态对开始**，这样每一对就有一个非零的被选概率；
   - 无法普遍依赖于它，尤其是当从与环境的实际交互中直接学习时；

4. **蒙特卡洛控制**：

   - 总体思想是依据广义策略迭代(GPI)进行，维护一个近似策略和一个近似价值函数；

   - 价值函数不断改善以更精确地近似当前策略的价值函数，当前策略也不断地完善；

   - 这两种改变某种程度上相互对立，但一起推进策略和价值函数达到最优性；

   - 经典策略迭代的MC版本，交替执行完整的评估和改善，最后得到最优策略和行为-价值函数；

   - 测量评估中，假设观测到无穷的节，每节都以探索启动产生，这样MC方法能为任何$v_k$计算确切的$q_{\pi_k}$；

   - 获得行为价值函数，策略改善选择当前状态的最大行为-价值函数，无需模型即可构建贪心策略；

   - 取消无穷个数节假设，的探索启动蒙特卡洛：
     $$
     \bbox[5px,border:2px solid]
     {\begin{aligned}
       &\underline{\mathbf{Monte\space Carlo\space ES\space(Exploring\space Starts),\space for\space estimating\space}\pi\approx\pi_*}\\
       \\
       &\text{Initialize, for all }s\in\mathcal S, a\in\mathcal A(s)\\
       &\qquad Q(s,a) \leftarrow \text{arbitrary}\\
       &\qquad \pi(s) \leftarrow \text{arbitrary}\\
       &\qquad Returns(s,a) \leftarrow \text{empty list}\\
       \\
       &\text{Repeat forever:}\\
       &\qquad \text{Choose }S_0\in\mathcal S\text{ and }A_0\in\mathcal A(S_0)\text{ s.t. all pairs have probability > 0}\\
       &\qquad \text{Generate an episode starting from }A_0,S_0,\text{ following }\pi\\
       &\qquad \text{For each pair }s,a\text{ appear in the episode:}\\
       &\qquad\qquad G \leftarrow \text{return following the first occurrence of }s,a\\
       &\qquad\qquad \text{Append }G\text{ to }Returns(s,a)\\
       &\qquad\qquad Q(s,a) \leftarrow \text{average}(Returns(s,a))\\
       &\qquad \text{For each }s\text{ in the episode:}\\
       &\qquad\qquad \pi(s) \leftarrow \arg\max_a Q(s,a)
     \end{aligned}}
     $$

5. **无ES的MC控制**：

   - on-policy方法尝试评估和改善用于决策的策略，如MCES；

   - off-policy方法则评估和改善一个不同于产生这些数据的策略；

   - on-policy方法中的策略通常是**松弛的(soft)**，这里移向一个$\varepsilon$-松弛算法：
     $$
     \bbox[5px,border:2px solid]
     {\begin{aligned}
       &\underline{\mathbf{On\text{-}policy\space first\text{-}visit\space MC\space control\space(for\space \varepsilon\text{-}soft\space policies),\space estimates\space\pi\approx\pi_*}}\\
       \\
       &\text{Initialize, for all }s\in\mathcal S, a\in\mathcal A(s)\\
       &\qquad Q(s,a) \leftarrow \text{arbitrary}\\
       &\qquad Returns(s,a) \leftarrow \text{empty list}\\
         &\qquad \pi(a\mid s) \leftarrow \text{an arbitrary }\varepsilon\text{-soft policy}\\
       \\
       &\text{Repeat forever:}\\
       &\qquad \text{(a) Generate an episode using }\pi\\
       &\qquad \text{(b) For each pair s,a appearing in the episode:}\\
       &\qquad\qquad G \leftarrow \text{return following the first occurrence of }s,a\\
       &\qquad\qquad \text{Append }G\text{ to }Returns(s,a)\\
       &\qquad\qquad Q(s,a) \leftarrow \text{average}(Returns(s,a))\\
       &\qquad \text{For each }s\text{ in the episode:}\\
       &\qquad\qquad A^* \leftarrow \arg\max_a Q(s,a)\\
       &\qquad\qquad \text{For all }a\in\mathcal A(s)\text{:}\\
       &\qquad\qquad\qquad \pi(a\mid s) \leftarrow 
       \begin{cases}
       1-\varepsilon+\varepsilon/\left\vert \mathcal A(s) \right\vert,&\text{ if }a=A^*\\
       \varepsilon/\left\vert \mathcal A(s) \right\vert, &\text{ if }a\neq A^*
       \end{cases}
     \end{aligned}}
     $$

6. **基于重要性采样的off-policy预测**：

   - 所有学习控制算法的困境：追求最优却得表现得非最优以探索最优；

   - 使用两个策略，一个是要学习的**目标策略**，一个是产生行为的**行为策略**；

   - On-policy方法更简单并首先考虑，Off-policy方法需额外概念和记号，且数据方差更大收敛更慢；

   - Off-policy更强大更普遍，包含on-policy，有很多额外用处，是学习世界动态多步预测模型的关键；

   - **包含假设**：若目标策略的$\pi(a\mid s)>0$，则行为策略的$b(a\mid s)>0$；

   - Off-policy方法需要用到**重要性采样**，一种给定其他分布样本来估计某分布下期望价值的通用技巧；

   - 给定开始状态$S_t$，随后的状态-行动轨迹$A_t,S_{t+1},A_{t+1}, \dots, S_T$在任意策略$\pi$下发生的概率是：
     $$
     \begin{aligned}
     &\qquad\text{Pr}\{ A_t,S_{t+1},A_{t+1},\dots,S_T \mid S_t,A_{t:T-1} \sim \pi \}\\
     &=\prod_{k=t}^{T-1}\pi(A_k\mid S_k)p(S_{k+1}\mid S_k, A_k)
     \end{aligned}
     $$

   - 在此目标和行为策略下轨迹的相对概率（**重要性采样率**）是：
     $$
     \rho_{t:T-1} = \frac{\prod_{k=t}^{T-1}\pi(A_k\mid S_k)p(S_{k+1}\mid S_k, A_k)}{\prod_{k=t}^{T-1}b(A_k\mid S_k)p(S_{k+1}\mid S_k, A_k)} = \prod_{k=t}^{T-1}\frac{\pi(A_k\mid S_k)}{b(A_k\mid S_k)}
     $$

   - 记$\mathcal T(s)$为访问状态s的所有时间步，$T(t)$为时间$t$后首次终止时间，$G_t$为$t$到$T(t)$回报；

   - 则$\{G_t\}_{t\in\mathcal T(s)}$是属于状态$s$的回报，而$\{\rho_{t:T(t)-1}\}_{t\in\mathcal T(s)}$是对应的重要性采样率；

   - **常规重要性采样**评估$v_\pi(s)$：
     $$
     V(s) \dot= \frac{\sum_{t\in\mathcal T(s)} \rho_{t:T(t)-1}G_t}{\left\vert \mathcal T(s) \right\vert}
     $$

   - **加权重要性采样**评估$v_\pi(s)$：
     $$
     V(s) \dot= \frac{\sum_{t\in\mathcal T(s)} \rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}}
     $$

   - 常规重要性采样估算是无偏的，而加权重要性采样估算则是有偏的（偏差渐近趋于0）；

   - 常规重要性采样估算的方差一般无界，加权的方差趋于0，实践中加权通常方差显著更低并强烈优先。