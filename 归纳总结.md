##### Ch2 多臂老虎机

1. **行动价值**：选择行动$a$后的期望激励$q_*(a)$：
   $$
   q_*(a) \dot= \mathbb E[R_t\mid A_t=a]
   $$

2. **行动-价值方法**：记行动$a$在时间$t$的估计价值为$Q_t(a)$，则
   $$
   Q_t(a) \dot= \frac{\text{迄时间t采取a获得激励的总和}}{\text{迄时间t采取a的次数}} = \frac{\sum_{i=1}^{t-1} R_i \bullet\mathbf 1_{A_i=a}}{\sum_{i=1}^{t=1} \mathbf 1_{A_i=a}}
   $$
   $\mathbf 1_{predicate}$为预测为真时1否则0的随机变量；若分母为0，则设$Q_t(a)$为一默认值，如$Q_1(a) = 0$。

3. **10臂试验台**：行动价值$q_*(a), a=1,\dots,10$由均值为0方差为1的正态分布产生；在时间$t$选择行动$A_t$后，激励值$R_t$由均值为$q_*(A_t)$方差为1的正态分布产生。

4. **贪婪与$\varepsilon$贪婪**：若某个行动激励的方差变大，适于$\varepsilon$-贪婪；若方差为0，则贪婪表现最好，但若任务非平稳，也需要探索。

5. **增量实现**：令$R_i$表示所评估的行动在第$i$次选中后收到的激励，$Q_n$表示被选中$n-1$次后价值的估计，即$Q_n \dot= \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}$，可以改为增量形式来提高效率：
   $$
   Q_{n+1} = \frac{1}{n} \sum_{i=1}^n R_i  = Q_n + \frac{1}{n} \left[ R_n - Q_n \right]
   $$

6. 使用增量计算样本均值和$\varepsilon$贪婪行为选择的完整老虎机算法：
   $$
   \bbox[25px,border:2px solid]
   {\begin{aligned}
   &\underline{\mathbf{A\space simple\space bandit\space algorithm}}\\
   \\
   &\text{Initialize, for }a=1\text{ to }k\text{:}\\
   &\qquad Q(a) \leftarrow 0\\
   &\qquad N(a) \leftarrow 0\\
   \\
   &\text{Repeat forever:}\\
   &\qquad A \leftarrow 
   \begin{cases}
   \arg\max_a Q(a)&\qquad\text{with probability }1-\varepsilon\\
   \text{a random action}&\qquad\text{with probability }\varepsilon
   \end{cases}\\
   &\qquad R \leftarrow bandit(A)\\
   &\qquad N(A) \leftarrow N(A) + 1\\
   &\qquad Q(A) \leftarrow Q(A) + \frac{1}{N(A)}[R-Q(A)]
   \end{aligned}}
   $$

7. 用**常量步长**处理**非平稳**问题：非平稳环境中行动的真值随时间变化，可以给近期的激励赋予更多权重，即加权平均，最普遍的一种做法是使用固定步长参数，也称指数新近加权均值：
   $$
   Q_{n+1} \dot= Q_n + \alpha \left[ R_n - Q_n \right]=(1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
   $$
   步长参数$\alpha \in (0,1]^1$为常量，权重之和$(1-\alpha)^n + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}=1$。

8. **步长参数随步数改变**：令$\alpha_n(a)$表示$n$次选择行动$a$后用于计算激励的步长参数，在随机逼近理论中，保证以概率1收敛到真实行动价值的条件是：
   $$
   \sum_{n=1}^\infty \alpha_n(a) = \infty\qquad\text{and}\qquad\sum_{n=1}^\infty \alpha_n^2(a) < \infty
   $$
   第一个条件保证了步长足够大到能逐渐克服任何初始条件或随机波动，这二个条件保证了步长逐渐变得足够小到能收敛。采样-平均的步长参数$\alpha_n(a) = \frac{1}{n}$两个收敛条件都满足；常量步长$\alpha_n(a)=\alpha$不满足第二个条件。



##### Ch3 有限马尔科夫过程

1. **$t$时刻的回报的定义**：
   $$
   \begin{eqnarray*}
   G_t
   &=& \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} &\quad T=\infty\ \text{and}\ \gamma=1\ \text{not simultaneously}\\
   &=& R_{t+1} + \gamma G_{t+1} &\quad \text{Bellman equation}
   \end{eqnarray*}
   $$

2. **马尔科夫特性**：若信号具有马尔科夫特性，则环境在$t+1$时刻的反应仅依赖于在时间$t$状态和行为的表达：
   $$
   P(S_{t+1},R_{t+1}|S_0,A_0,R_1,S_1,A_1,\dots,R_t,S_t,A_t) = P(S_{t+1}, R_{t+1} \mid S_n, A_n)
   $$

3. **马尔科夫决策过程**：

   - 满足马尔科夫特性的强化学习任务；若状态和空间都有限，为有限MDP；
   - $p(s',r\mid s,a)$可完全指定一个有限MDP动态，计算有关环境的任何内容；

4. **策略$\pi$的状态-价值函数**，用于评估状态的好坏：
   $$
   v_{\pi}(s) \dot=\mathbb E_{\pi}[G_t | S_t=s] = \mathbb E_\pi \left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \middle| S_t=s \right]
   $$

5. **策略$\pi$的行为-价值函数**：用于评估某状态下某行为的好坏：
   $$
   q_{\pi}(s,a) \dot= \mathbb E_{\pi}[G_t \mid  S_t=t, A_t=a] = \mathbb E_{\pi}\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \middle | S_t=s, A_t=a \right]
   $$

6. **$v_\pi$的贝尔曼方程**：表述状态的价值与其后继价值的关系：
   $$
   v_\pi(s) = \sum_a \pi(a\mid s) \sum_{s',r} p(s',r\mid s,a)\left[ r+\gamma v_\pi(s') \right]\quad\forall s \in \mathcal S
   $$

7. **$q_\pi$的贝尔曼方程**：
   $$
   q_\pi(s,a) = \sum_{s',r}p(s',r\mid s,a) \left[ r + \gamma\sum_{a'}\pi(a'\mid s')q_\pi(s',a') \right]\quad \forall s\in\mathcal S, \forall a\in\mathcal A(s)
   $$

8. **$v_\pi$与$q_\pi$的关系**：
   $$
   \begin{eqnarray*}
   v_\pi(s) &=& \sum_a \pi(a\mid s) q_\pi(s,a)\\
   q_\pi(s,a) &=& \sum_{s',r}p(s',r\mid s,a)\left(r+\gamma v_\pi(s')\right)
   \end{eqnarray*}
   $$

9. **最优状态价值函数$v_*$**：
   $$
   v_*(s) \dot= \max_\pi v_\pi(s),\quad \forall s \in \mathcal S
   $$

10. **最优行为价值函数$q_*$**：
  $$
  q_*(s,a) \dot= \max_{\pi} q_\pi(s,a)\quad \forall s \in \mathcal S,\forall a\in \mathcal A(s)
  $$

11. **$v_*$最优贝尔曼性方程**：
    $$
    v_*(s) = \max_{a\in\mathcal A(s)} \sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma v_*(s') \right]
    $$

12. **$q_*$最优贝尔曼性方程**：
    $$
    q_*(s,a) = \sum_{s',r} p(s'r\mid s,a) \left[ r+\gamma\max_{a'}q_*(s',a') \right]
    $$

13. **$v_*$与$q_*$的关系**：
    $$
    \begin{eqnarray*}
    v_*(s) &=& \max_a q_*(s,a)\\
    q_*(s,a)&=& \sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma v_*(s') \right]
    \end{eqnarray*}
    $$









##### Ch4 动态规划

1. **迭代策略评估**：若环境动态$p(s',r\mid s,a)$已知，则$v_\pi$的贝尔曼方程就是$\left\vert\mathcal S\right\vert$个$\left\vert\mathcal S\right\vert$元线性方程组，用迭代方法计算，$v_0, v_1, v_2,\cdots$为近似价值函数序列初始估计$v_0$可任意选择（除终止状态必须为0），对任意$s \in \mathcal S$:
   $$
   \begin{eqnarray*}
   v_{k+1}(s)
   &=& \mathbb E_\pi[R_{t+1}+\gamma v_k(S_{t+1}) \mid S_t=s] \\
   &=& \sum_a \pi(a\mid s)\sum_{s',r} p(s',r\mid s,a) \left[ r+\gamma v_k(s') \right]
   \end{eqnarray*}
   $$

2. 带**停机标准**的完整策略评估迭代算法为：
   $$
   \bbox[25px,border:2px solid]
   {
   \begin{aligned}
   &\underline{\mathbf{Iterative\space policy\space evaluation}}\\
   \\
   &\text{Input }\pi,\text{ the policy to be evaluated}\\
   &\text{Initialize an array }V(s)=0,\text{ for all }s\in\mathcal S^+\\
   &\text{Repeat}\\
   &\qquad\Delta \leftarrow 0\\
   &\qquad\text{For each }s\in\mathcal S:\\
   &\qquad\qquad v \leftarrow V(s)\\
   &\qquad\qquad V(s)\leftarrow\sum_a\pi(a\mid s)\sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma V(s') \right]\\
   &\qquad\qquad\Delta\leftarrow\max(\Delta,\left\vert v-V(s) \right\vert)\\
   &\text{until }\Delta < \theta\text{ (a small positive number) }\\
   &\text{Output }V\approx v_\pi
   \end{aligned}
   }
   $$

3. **策略改善定理**：若$\pi$和$\pi'$为任意策略对，若$\forall s \in \mathcal S$有
   $$
   q_\pi(s,\pi'(s)) \ge v_\pi(s)
   $$
   则策略$\pi'$必然与策略$\pi$等优或更优，即$\pi$必然从$\forall s \in \mathcal S$获得更多或相等的期望回报,
   $$
   v_\pi'(s)\ge v_\pi(s)
   $$
   若对任意状态前一个方程都是严格不等式，则后一个方程中至少有一个状态是严格的。要证明策略改善定理，从前一个不等式开始持续展开$q_\pi$侧并反复应用它直到获得$v_{\pi'}(s)$。

4. **策略改善**：已知策略及其价值函数，由策略改善定理，在每个状态使用贪婪策略——即选择每个状态$q_\pi(s,a)$最好的行为——来改善策略：
   $$
   \begin{eqnarray*}
   \pi'(s)
   &\dot=&  \arg\max_a q_\pi(s,a)\\
   &=& \arg\max_a \sum_{s',r} p(s',r\mid s,a)\left[ r+\gamma v_\pi(s') \right]
   \end{eqnarray*}
   $$
   改进后的策略$\pi'$，满足$\forall s \in \mathcal S$，
   $$
   \begin{eqnarray*}
   v_{\pi'}(s)
   &=& \max_a \mathbb E\left[ R_{t+1} + \gamma v_{\pi'}(S_{t+1}) \middle | S_t=s, A_t=a \right]\\
   &=& \max_a \sum_{s',r} p(s',r\mid s,a) \left[ r+\gamma v_{\pi'}(s') \right]
   \end{eqnarray*}
   $$
   满足贝尔曼最优性方程，因此$\pi'$必然是$\pi^*$。策略改善必然会给出严格更优的策略，除非原策略已是最优。

5. **策略迭代**：通过计算价值函数$v_\pi$改善策略$\pi$得到策略$\pi'$，又能计算$v_{\pi'}$产生更好的$\pi''$，获得一个单调递增的策略和价值函数序列：
   $$
   \begin{CD}
   \pi_0 @>E>> v_{\pi_0} @>I>> \pi_1 @>E>> v_{\pi_1} @>I>> \pi_2 @>E>> \cdots @>I>> \pi_* @>E>> v_*
   \end{CD}
   $$
   每个策略都保证为前一个的严格改善（除非已是最优），因有限MDP仅策略有限，它必然会在有限次迭代后收敛到最优策略和最优价值函数。完整算法如下：
   $$
   \bbox[25px,border:2px solid]
   {
   \begin{aligned}
   &\underline{\mathbf {Policy\space iteration}}\\
   \\
   1.&\text{Initialization}\\
   &V(s) \in \mathbb R\text{ and }\pi(s) \in \mathcal A(s)\text{ arbitrary for all }s\in \mathcal S\\
   \\
   2.&\text{Policy Evaluation}\\
   &\text{Repeat}\\
   &\qquad\Delta \leftarrow 0\\
   &\qquad\text{For each }s \in \mathcal S:\\
   &\qquad\qquad v\leftarrow V(s)\\
   &\qquad\qquad V(s) \leftarrow \sum_{s',r}p(s',r\mid s,\pi(s))\left[ r+\gamma V(s') \right]\\
   &\qquad\qquad \Delta\leftarrow \max(\Delta,\left\vert v-V(s) \right\vert)\\
   &\text{until }\Delta<\theta\text{ (a small positive number) }\\
   \\
   3.&\text{Policy Improvement}\\
   &policy\text-stable \leftarrow true\\
   &\text{For each }s\in\mathcal S:\\
   &\qquad old\text-action \leftarrow \pi(s)\\
   &\qquad \pi(s) \leftarrow \arg\max_{a} \sum_{s',r} p(s',r\mid s,a)\left[ r+\gamma V(s') \right]\\
   &\qquad \text{If } old\text-action\neq\pi(s),\text{ then }policy\text-stable \leftarrow false\\
   &\text{If }policy\text-stable,\text{ then stop and return }V\approx v_*\text{ and }\pi\approx\pi_*;\text{else go to 2}
   \end{aligned}
   }
   $$

6. **价值迭代**：在一次备份后就停止评估可以在不失去收敛保证下缩短策略迭代的策略评估，能够写为一个特别简单的结合策略改善和缩短策略评估的备份操作：
   $$
   \begin{eqnarray*}
   v_{k+1}(s) 
   &=& \max_a \mathbb E\left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t=s, A_t=a \right]\\
   &=& \max_a \sum_{s', r} p(s',r\mid s,a)\left[ r+\gamma v_k(s') \right]
   \end{eqnarray*}
   $$
   完整算法为：
   $$
   \bbox[25px,border:2px solid]
   {\begin{aligned}
   &\underline{\mathbf{Value\space iteration}}\\
   \\
   &\text{Initialize array }V\text{ arbitrarily}\\
   \\
   &\text{Repeat}\\
   &\qquad \Delta \leftarrow 0\\
   &\qquad\text{For each }s \in \mathcal S:\\
   &\qquad\qquad v \leftarrow V(s)\\
   &\qquad\qquad V(s) \leftarrow \max_a\sum_{s',r}p(s',r\mid s,a)\left[ r+\gamma V(s') \right]\\
   &\qquad\qquad \Delta \leftarrow \max(\Delta, \left\vert v-V(s) \right\vert)\\
   &\text{until }\Delta < \theta\text{ (a small positive number)}\\
   &\\
   &\text{Output a deterministic policy, }\pi \approx\pi_*,\text{ such that}\\
   &\qquad\pi(s) = \arg\max_a \sum_{s',r} p(s',r\mid s,a)\left[ r + \gamma V(s') \right]
   \end{aligned}}
   $$

7. **异步动态规划**：

8. **广义**




##### Ch5 蒙特卡洛方法

1. **蒙特卡洛方法**：基于平均样本回报来解决强化学习问题的方法；经验被分为小节，而无论选择什么行为所有小节最终都会终止；仅当一个小节结束时，价值评估和策略才会改变，因此MC方法是**按节**递增。