**练习3.1** 设计三个适用于强化学习框架的任务，确定每一个的状态、任务和激励。使得它们尽可能地不同。

**练习3.2** 强化学习框架是否足够能有效地表示所有目标导向的学习任务？能举出任何反例吗？

**练习3.3** 考虑驾驶问题，可定义行为是加速器、方向盘、刹车，即身体接触机器的地方；也可以是轮胎扭矩，即远离身体的外部；或者是肌肉扭动控制身体，即深入身体的内部；还可以是驶向何方的选择。什么才是正确的层次？基于何种基础相比其它更好？是否有解释一个位置比另一个更好的基本理由，或只是自由选择？

**练习3.4** 假设视平衡杆为分节任务，但依然使用失败为-1其余为0激励的折扣。则每次的回报是什么？与此任务的连续折扣形式的区别是什么？

**练习3.5** 设计一个走迷宫机器人，给逃出迷宫激励为+1，其余为0。任务似乎自然分节——每次成功逃脱——目标就是最大化总激励的期望。在运行学习代理一段时间后发现对逃出迷宫没有任何改进，问题出在哪里？与代理有效沟通了需要实现的目标了吗？

**解答**：激励设置地有问题。

对未逃离迷宫的每一步施加负激励。

**练习3.6** 假设$\gamma=0.5$，并接收到如下激励序列：$R_1=-1, R_2=2, R_3=6, R_4=3, R_5=2$，其中$T=5$。则$G_0, G_1, \dots, G_5$分别是什么？提示：回溯。

注意，尽管(3.2)的回报是无穷项之和，但若激励非0且不变，其值依然有界。如激励恒为+1，则回报为：
$$
G_t = \sum_{k=0}^\infty \gamma^k = \frac{1}{1-\gamma} \tag{3.4}
$$
**练习3.7** 假设$\gamma=0.9$，激励序列是$R_1=2$后接无穷的7，则$G_1$、$G_0$是什么？

**练习3.8** *破损视觉系统*：想象一个视觉系统，第一次打开时，图像洪流涌入摄像头中，可以看到很多事物，但并非全部，比如闭塞或身后的物体。在看到第一个场景后，系统是否能获得环境马尔科夫的状态？若那天摄像头破损并为获得任何图像，则还能获得马尔科夫状态吗？

