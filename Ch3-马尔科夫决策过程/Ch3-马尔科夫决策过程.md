本章介绍本书剩余部分所要解决的问题。可以认为这个问题定义了强化学习的领域：任何适于解决此问题的方法都认为是一种强化学习方法。

##### 3.1 代理-环境接口

学习者和决策者称为*代理(agent)*，与它互动的、代理以外所有的事物统称为*环境(enviroment)*。一个完整的环境明细(specification)包括如何确定激励(reward)、定义*任务*，和一个强化学习问题的实例。代理和环境在一个离散时间步序列，$t=0,1,2,3,\cdots$，上互动。在每个时间步$t$，代理获得一个环境*状态(state)*的表示，$S_t \in \mathcal S$，$\mathcal S$是所有可能的状态；然后以状态为基础选择一个*行为(action)*，$A_t \in \mathcal A(S_t)$，$\mathcal A(S_t)$表示在状态$S_t$能获得的行为集。一个时间步后，作为行为结果的一部分，代理收到一个数值激励，$R_{t+1} \in \mathcal R \subset \mathbb R$，并发现自己处于新状态$S_{t+1}$中。下图展示了这种代理-环境的互动。

<img src="note3_pics/agt-env-int.png" width="600px" text-align="middle">

在每个时间步，代理执行从状态到选择每个行为概率的映射，这种映射称之为代理的*策略(policy)*，并记为$\pi_t$，其中$\pi_t(a|s)$表示若状态$S_t=s$行为$A_t=a$的概率。作为经验的结果，强化学习方法指定了改变策略的方式。代理的目标就是最大化其长期的激励总量。

这个框架非常抽象和灵活，可以以不同的方式应用到不同的问题。比如，时间步不必是真实时间的固定间隔，可以是任意决策或行为的连续状态；行为和状态都可以采用非常多的形式；特别是，代理和环境的边界并非如机器人或动物身体的物理边界那样，通用的原则是任何无法被代理随意改变的事物都认为在代理之外即环境的一部分，也可以因为不同的原因设置在不同位置。

##### 3.2 目标和激励

代理的目标是最大化获得的总激励的数目，这表明不是即时激励，而是长期累积的激励。形式化的思想可以用*激励假设(reward hypothesis)*来清楚地阐述：

> 所有关于目标(goals)或目的(purposes)含义都可以认为是激励信号累积值期望的最大值。

使用激励信号来形式化目标的思想，是强化学习最独特的特征之一，并已经证明足够地灵活和有效。需要注意虽然大多数动物最终目标感知的计算都发生在机体内，但仍然认为目标和激励处于代理之外，属于环境。因为**将学习代理的边界置于其控制的极限而非物理身体的极限非常方便**，而这并不会妨碍代理为自己定义内部激励。

##### 3.3 回报

那如何正式地定义代理的目标呢？假设时间步$t$后的激励序列为$R_{t+1}, R_{t+2}, R_{t+3},\cdots$,通常会寻求最大化*期望的回报(expected return)*，其中的回报(return)定义为激励序列的特定函数。最简单的方式就是定义为激励之和：
$$
G_t \dot=R_{t+1} + R_{t+2} + \cdots + R_T \tag{3.1}
$$
其中$T$是结束的时间步。这种方法在时间步自然结束的应用有效，即代理-环境的互动可以自然地分为称之为*节(episodes)*的子序列，比如比赛、迷宫旅行以及任何其它重复的互动形式，每一节以特殊的*终止状态(terminal state)*结束，然后重置为标准的开始状态或开始状态分布的采样，即便以不同的方式结束，下一节的开始依然与前一节的结束方式无关，因此所有的节都可以认为是以相同的终结状态结束，只是不同的结果有不同的激励。有节的任务都被认为是*分节任务(episodic tasks)*，有时这种任务需要从所有状态$\mathcal S^+$中区分出所有非终结状态$\mathcal S$。

但也有很多无限运行的*持续任务(continuing task)*就无法使用(3.1)的回报公式，因$T = \infty$，要最大化的回报也很易是无限大。这里引入*折扣(discounting)*的概念，即代理尝试选择使得以后折扣激励之和最大的行为。特别地，它选择$A_t$来最大化期望的折扣回报：
$$
G_t \dot=R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots  = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \tag{3.2}
$$
其中参数$\gamma \in [0,1]$称为折扣速率，它决定了未来激励的当前值：即**未来$k$步获得的激励仅是其即时获得的$\gamma^{k-1}$倍**。若$k<1$，则只要激励序列$R_k$是有界的，这个无限的和就有有限的值。若$\gamma=0$，则代理就是近视的，仅关心最大的即时激励；若代理的所有行为仅影响即时利益，则近视代理就能通过分别最大化每个激励来最大化(3.2)；但在一般情况下，这样就减少了获得未来激励的机会，因此事实上的回报会减小；随着$\gamma$接近1，目标就越来越重视未来的激励，代理就变得越来越有远见。

##### 3.4 统一记号 

前面说到强化学习任务有分节任务和连续任务两种，但在讨论分节任务时并不会区分不同的节，因此可以将每一节的结束视为进入特殊的只会转移到自身且激励为0的终止状态，比如：

<img src="note3_pics/epis-task.png" width="500px" text-align="middle" />

这样就能将两者统一起来，甚至在引入折扣的时候也正确，因此回报可以写为：
$$
G_t \dot= \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} \tag{3.5}
$$
包括了$T = \infty​$和$\gamma=1​$的情况（但不同时成立）。

##### 3.5 马尔科夫性质

本书中，“状态”表示任何代理能获得的信息，并假定由环境给出；即这里主要关注如何用获得的激励做决策而非设计状态信号。状态的表达可以是原始感知的高度处理版，也可以是从感知序列按时间建立的复杂结构。另一方面，也不应该期待状态信号包含关于环境的所有信息，即便对决策有用。理想的状态信号应该是能简洁归纳过去感知、还能保留所有相关信息。成功地包含所有相关信息的状态信号称为是马尔科夫的，或者具有马尔科夫特性。

对于一般的环境，在时间$t+1$对时间对时间$t$采取的行为$a$的反应，仅能由整个联合概率分布给出，即对所有的$r$，$s'$和所有过去事件$S_0,A_0,R_1,\dots,S_{t-1}, A_{t-1},R_t,S_t,A_t$可能的值，有
$$
\text{Pr}\{S_{t+1}=s', R_{t+1}=r | S_0, A_0, R_1, \dots,S_{t-1}, A_{t-1}, R_t,  s_t, A_t\} \tag{3.6}
$$
若状态信号具有马尔科夫特性，环境在时间$t+1$的反映仅依赖于在时间$t$状态和行为的表达，即对所有$r$，$s'$，$s$和$a$，有：
$$
p(s',r|s,a) \dot = \text{Pr}\{S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a\} \tag{3.7}
$$
这种情况下，环境和任务也被称为具有马尔科夫特性。

在具有马尔科夫特性的环境中，单步动力学（公式3.6）使得在给定当前状态和行为时能够预测下一个状态和下一个激励期望。迭代这个公式，仅以当前状态和可能给出的目前为止的历史，就可以预测所有未来的状态和激励期望。即便状态信号为非马尔可夫的，将强化学习中的状态视为马尔科夫状态的近似也很有用。

马尔可夫特性在强化学习中非常重要，因为决策和价值被假设为仅是当前状态的函数，因此状态必须具有信息价值。本书中所有的理论都假设了马尔可夫状态信号，即便马尔可夫特性并非严格满足的情况，马尔可夫情形依然能帮助理解应用在这些情况中的算法。