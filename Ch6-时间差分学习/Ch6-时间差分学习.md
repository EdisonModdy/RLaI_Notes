若希望确认一个思想对强化学习是中心或新颖，则它毫无疑问是**时间差分学习(temporal-difference or TD learning)**。TD学习是蒙特卡洛(MC)思想和动态规划(DP)思想的结合。像MC，TD方法也能不靠环境动态的模型从原始经验中学习；像DP，TD方法更新估计部分基于其他学得的估计，而无需等待一个最终的结果——它们引导启动(bootstrap)。TD、DP和MC的关系是强化学习理论中循环的主题，本章是探索它的起点，在结束以前，我们会看到这些思想和方法相互融合并能够以许多方法组合起来。尤其是，在第7章会介绍$n$-步算法，它构建了从TD到MC方法的桥梁；在第12章会介绍$\text{TD}(\lambda)$算法，它将它们无缝地统一了起来。

如往常一样，从关注策略评估或预测问题开始，也即给定策略$\pi$估计价值函数$v_\pi$。对于控制问题（找到最优策略），DP、TD和MC方法都使用了广义策略迭代(GPI)的某种变形。这些方法间的差异首先是它们要解决的预测问题的差异。