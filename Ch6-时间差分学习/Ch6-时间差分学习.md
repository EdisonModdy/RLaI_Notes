若希望确认一个思想对强化学习是中心或新颖，则它毫无疑问是**时间差分学习(temporal-difference or TD learning)**。TD学习是蒙特卡洛(MC)思想和动态规划(DP)思想的结合。像MC，TD方法也能不靠环境动态的模型从原始经验中学习；像DP，TD方法更新估计部分基于其他学得的估计，而无需等待一个最终的结果——它们引导启动(bootstrap)。TD、DP和MC的关系是强化学习理论中循环的主题，本章是探索它的起点，在结束以前，我们会看到这些思想和方法相互融合并能够以许多方法组合起来。尤其是，在第7章会介绍$n$-步算法，它构建了从TD到MC方法的桥梁；在第12章会介绍$\text{TD}(\lambda)$算法，它将它们无缝地统一了起来。

如往常一样，从关注策略评估或预测问题开始，也即给定策略$\pi$估计价值函数$v_\pi$。对于控制问题（找到最优策略），DP、TD和MC方法都使用了广义策略迭代(GPI)的某种变形。这些方法间的差异首先是它们要解决的预测问题的差异。



##### 6.1 TD预测

TD和MC方法都使用经验来解决预测问题。给定一些遵循策略$\pi$的经验，两种方法都为每个出现在此经验中的非终止状态$S_t$更新其$v_\pi$的估计$V$。大致而言，MC方法等到已知访问后的回报才使用此回报作为$V(S_t)$的目标。一个简单的适用于与非平稳环境的每访MC方法为：
$$
V(S_t) \leftarrow V(S_t) + \alpha\Big[G_t-V(S_t)\Big]\tag{6.1}
$$
其中$G_t$是时间$t$后的实际回报，$\alpha$是常数步长参数，称这种方法为**constant-$\alpha$ MC**。相比MC方法必须等到节的终结才确定$V(S_t)$的增量（直到那时$G_t$才已知），TD方法仅需等到下个时间步。在时间$t+1$它们立即就能使用观测到的激励$R_{t+1}$和估计$V(S_{t+1})$来构造一个目标并做出有用的更新。最简单的TD方法即刻便在到$S_{t+1}$的转移和获得的回报$R_{t+1}$上更新：
$$
V(S_t) \leftarrow V(S_t) + \alpha\Big[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \Big]\tag{6.2}
$$
实际上，MC更新的的目标是$G_t$，而TD更新的目标是$R_{t+1}+\gamma V(S_{t+1})$。这种TD方法称为**TD(0)**或**一步TD(one-step TD)**。因其是第7和12章扩展的**TD($\lambda$)**和**多步TD**方法的特例。下面以程序的形式详细完整地阐述了TD(0)：
$$
\bbox[5px,border:2px solid]
{\begin{aligned}
  &\underline{\mathbf{Tabular\ TD(0)\ for\ estimating\ }v_\pi}\\
  \\
  &\text{Input: the policy }\pi\text{ to be evaluated}\\
  &\text{Initialize }V(s)\text{ arbitrary }(\text{e.g., }V(s)=0,\ \forall s \in \mathcal S^+)\\
  &\text{Repeat (for each episode):}\\
  &\qquad \text{Initialize }S\\
  &\qquad \text{Repeat (for each step of episode):}\\
  &\qquad\qquad A \leftarrow \text{action given by }\pi\text{ for }S\\
  &\qquad\qquad \text{Take action }A,\text{ observe }R, S'\\
  &\qquad\qquad V(s) \leftarrow V(s) + \alpha\left[ R + \gamma V(S') - V(S) \right]\\
  &\qquad\qquad S \leftarrow S'\\
  &\qquad\text{until }S\text{ is terminal}\\
\end{aligned}}
$$
因TD(0)将更新部分基于一个存在的估计，称它是一种**自举(bootstrapping)**的方法，就像DP。从第3章可知：
$$
\begin{eqnarray*}
v_\pi(s) &\dot=& \mathbb E_\pi\left[ G_t \mid S_t=s \right]\tag{6.3}\\
&=& \mathbb E_\pi\left[ R_{t+1} + \gamma G_{t+1} \mid S_t=s \right]\tag{from (3.3)}\\
&=& \mathbb E_\pi\left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \right]\tag{6.4}
\end{eqnarray*}
$$
大体而言，MC方法使用(6.3)的估计作为目标，而DP方法则使用(6.4)的估计作为目标。蒙特卡洛目标是一个估计因(6.3)中的预期的值未知，用一个样本回报来替代真实预期的回报。DP目标是估计并非因为预期的值，这假定完全由环境的模型提供，而是因为$v_\pi(S_{t+1})$，而使用了当前的估计$V(S_{t+1})$。TD目标是估计原因包含上面两个：它采样了(6.4)中的预期值并使用了当前估计$V$而非真实$v_\pi$。因此，TD方法将MC采样和DP自举结合了起来。就像我们将看到的，通过审慎和想象，这能在获得MC和DP两种方法的优势上取得长足的进展。

下面图表是表格TD(0)的备份图。备份图顶端状态节点的价值估计基于从它出发即时后续状态转移的样本来更新。我们认为TD和MC更新是**样本备份(sample backup)**，因为它们包含了向前一步的后继状态（或状态-行动对）的抽样，使用沿途后继者的价值和激励来计算一个备份值，然后相应原始状态（或状态-行动对）的价值。样本备份不同于DP的全备份在于它们基于单个后继样本而非所有可能后继者的完整分布。

最后，注意TD(0)中的数量是一种误差，它衡量$S_t$的估计值与更好估计$R_{t+1}+\gamma V(S_{t+1})$之间差异。这个称为**TD误差(TD error)**的数量自强化学习始终以多种形式出现：
$$
\delta_t \dot= R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\tag{6.5}
$$
注意每个时间TD误差那个时间做估计的误差。因为TD误差依赖于下个状态和下个激励，因此要在一个时间步后才能获得。也就是说$\delta_t$是在时间$t+1$获得的$V(S_t)$的误差。同样注意若数组$V$在节内没有改变（就像在MC方法中没有改变一样），则MC误差可以写为TD误差的和：
$$
\begin{eqnarray*}
G_t-V(S_t)
&=& R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1})  - \gamma V(S_{t+1})\tag{from (3.3)}\\
&=& \delta_t + \gamma\left( G_{t+1} - V(S_{t+1}) \right)\\
&=& \delta_t + \gamma\delta_{t+1} + \gamma^2\left( G_{t+2}-V(S_{t+2}) \right)\\
&=& \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}\left(G_T-V(S_T)\right)\\
&=& \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(0-0)\\
&=& \sum_{k=t}^{T-1} \gamma^{k-t}\delta_k\tag{6.6}
\end{eqnarray*}
$$
若在节内$V$改变了（就像在TD(0)中）这个等式就不再准确，但若步长很小则它依然近似成立。这个等式的推广在TD学习的理论和算法中十分重要。

**练习6.1**：若$V$在节内改变，则(6.6)仅近似相等。则两边之间的差异是什么？令$V_t$表示在时间$t$TD误差(6.5)和TD更新(6.2)中使用的状态价值数组，重做上面的推导以确定要等于MC误差TD误差需要增加的额外量。

**示例6.1 开车回家**：每天下班开车回家时，你会试着预测需要耗费的时间。当离开办公室时，你会注意时间、星期、天气以及其他任何有关的事物。假定本周五你在恰好下午6点时离开办公室，并估计需要花30分钟到家。在你到车旁时是6:05，然后你注意到开始下雨了。雨中交通通常更慢，因此你重新估计此后会花35分钟，或总共40分钟。15分钟后你及时地完成旅途的主路部分，当你离开进入辅路时，你将总的预估时间减为35分钟，但这时被堵在一辆很慢的卡车后面，路很窄无法超车，最终不得不跟在卡车后面直到在6:40你转到你居住小巷，三分钟之后到家。状态、时间和预测序列因此就是：

|     状态     | 过去的时间（分钟） | 预测去的时间 | 预测的总时间 |
| :--------: | :-------: | :----: | :----: |
| 周五六点，离开办公室 |     0     |   30   |   30   |
|   到车上，下雨   |     5     |   35   |   40   |
|    出主路     |    20     |   15   |   35   |
|   辅路，卡车后   |    30     |   10   |   40   |
|   进入居住街道   |    40     |   3    |   43   |
|     到家     |    43     |   0    |   43   |

本例的激励是在每段旅程耗去的时间（因只关注预测，方便起见这里用正值），因此每个状态的回报是从这个状态到家还要消耗的实际时间，每个状态的价值是到家的期望时间。第二列的数字给出了每个状态的当前估计值。观察MC方法操作的简单途径是在序列上画出预计的总时间，如图6.1左边。箭头显示了$\alpha=1$的常数-$\alpha$MC方法(6.1)推介预测的变化。这些恰好是每个状态估计价值（实际到家要的时间）和实际回报（实际到家要的时间）间的误差。在每个事件中，改变必须在线下做出，也就是你回到家后。

<img src="note6_pics/MC-TD-operation.png" width="750" text-align="middle" />

那么在学习开始之前一定要等到最终的结果出来吗？假定另一天你在离开办公室时又估计需要30分钟开车回家，但陷在严重的交通堵塞中，离开办公室25分钟后依然堵在主路上，现在估计需要另外的25分钟到家，总共50分钟。在等待交通时，你已知之前30分钟的估计太过乐观。你还必须等到家才提高你对最初状态的估计吗？按照MC方法是必须的，因为还未知道真实的回报。

但按照TD方法，可以立即学习，将你原始的估计从30分钟改变到50。实际上，每个估计都会转向紧随其后即时的估计。回到开车的第一天，图6.1右边展示了TD规则(6.2)推介推测的变化（这是$\alpha=1$规则做出的变化）。每个误差成比例于随时间预测的变化，也就是预测中的**时间差分**。



##### 6.2 TD预测方法的优势

TD方法部分以其他估计作为学习估计的基础。它们从一个猜想学习一个猜想——它们**自举(bootstrap)**。那它是好的事吗？TD方法相对于MC和DP方法有优势吗？详述和回答这个问题需要剩余的这本书乃至更多。本节简要地展望一些答案。

很明显，就无需环境、激励和下个状态概率分布方面，TD方法相对于DP方法具有优势。

TD方法下一个相对MC方法最明显的优势是它们能自然以一种在线、全增量的方式实现。而用MC方法则必须等到一节的结束，因为到那时回报才已知，而用TD方法就仅需等一个时间步。事实证明它这是一种惊人普遍的关键原因。一些应用的节非常长，将所有学习推迟到节的结束就太慢了。其他的应用则是连续任务根本并没有节。最后，就像前一章所注意到的，一些MC方法必须对采取实验行动的节忽视或打折，这会极大地降低学习的速度。TD方法则不容易受这些问题的影响因为它们从每个转移中学习而与后面采取的行为无关。

但TD方法是可靠的吗？从下个猜想学习一个猜想而无需等待事实的结果当然很方便，但还能保证收敛到正确的答案吗？幸运的是，答案是肯定的。对任意固定的策略$\pi$，TD(0)已被证明收敛到$v_\pi$，对于一个充分小的固定步长参数，和在补偿参数依照通常的随机近似条件(2.7)递减事以1的概率收敛。大多证明仅适用于(6.2)上面展示算法的基于表格的情况，但有一些适用于广义线性函数近似的情况。结果会在第9章中以更普遍的设定讨论。

既然TD和MC方法都能渐近地收敛到正确的预测，则很自然下一个问题就是“那一个更快地到那？”换句话说就是哪种方法学习得更快。哪种更高效地有限数据？目前这还是一个开放的问题，目前还没有人能在数学上证明一种方法比另一种收敛地更快。实际上，甚至以正式方式陈述这个问题的最合适的方式都还不确定。但实际中，通常可以发现在随机任务上TD方法比常数-$\alpha$MC方法更快收敛，就像图6.2所展示的。

<img src="note6_pics/Convergence.png" width="750px" text-align="middle" />

**练习6.2**：这是一个帮助你建立TD方法通常比MC方法更高效的直觉的